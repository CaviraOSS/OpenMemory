# PostgreSQL + pgvector Multi-Tenant Setup Guide

This guide walks you through upgrading OpenMemory to use PostgreSQL with pgvector for multi-tenant, production-scale deployments capable of handling 50M+ memories.

## üéØ Overview

**What you'll achieve:**
- ‚úÖ Multi-tenant architecture with tenant isolation
- ‚úÖ Native pgvector similarity search (< 50ms queries at 50M memories)
- ‚úÖ HNSW indexes for O(log n) performance
- ‚úÖ Supabase-ready configuration
- ‚úÖ Horizontal scalability

**Performance comparison:**

| Metric | Before (BYTEA + In-Memory) | After (pgvector + HNSW) |
|--------|----------------------------|-------------------------|
| 50M vector search | ‚ùå Impossible (OOM) | ‚úÖ < 100ms |
| Memory footprint | ‚ùå ~80GB RAM | ‚úÖ ~2GB RAM |
| Concurrent tenants | ‚ùå Limited | ‚úÖ Unlimited |
| Insert throughput | ‚úÖ 1000/s | ‚ö†Ô∏è 200-500/s |

## üìã Prerequisites

### 1. PostgreSQL with pgvector

**Supabase (Recommended):**
- pgvector is pre-installed on all Supabase projects ‚úÖ
- No additional setup required

**AWS RDS:**
```sql
-- Enable in Parameter Group
rds.enable_pgvector = 1

-- Then in your database:
CREATE EXTENSION vector;
```

**Self-Hosted:**
```bash
# Ubuntu/Debian
sudo apt install postgresql-15-pgvector

# macOS
brew install pgvector

# Or build from source
git clone https://github.com/pgvector/pgvector.git
cd pgvector
make
sudo make install
```

### 2. Node.js Dependencies

Already included in `package.json`:
- `pg` - PostgreSQL client
- `@types/pg` - TypeScript types

### 3. Database Credentials

Set these environment variables:
```bash
OM_PG_HOST=your-db-host.supabase.co
OM_PG_PORT=5432
OM_PG_DB=postgres
OM_PG_USER=postgres
OM_PG_PASSWORD=your-secure-password
OM_PG_SCHEMA=public
```

## üöÄ Migration Process

### Step 1: Backup Your Data

**CRITICAL:** Always backup before migrating!

```bash
# Full database backup
pg_dump -h $OM_PG_HOST -U $OM_PG_USER -d $OM_PG_DB > backup_$(date +%Y%m%d).sql

# Or just the OpenMemory tables
pg_dump -h $OM_PG_HOST -U $OM_PG_USER -d $OM_PG_DB \
  -t openmemory_memories \
  -t openmemory_vectors \
  -t openmemory_waypoints \
  > openmemory_backup.sql
```

### Step 2: Review Migration Scripts

Migration scripts are in `/packages/openmemory-js/migrations/`:

1. **001_enable_pgvector.sql** - Enables pgvector extension
2. **002_add_tenant_id.sql** - Adds tenant_id to all tables
3. **003_convert_vectors_to_pgvector.sql** - Converts BYTEA ‚Üí vector type
4. **004_update_primary_keys.sql** - Fixes PKs for multi-tenancy
5. **005_create_indexes.sql** - Creates HNSW vector indexes

**Review each file** before running to understand the changes.

### Step 3: Run Migrations

```bash
cd packages/openmemory-js

# Install dependencies
npm install

# Run migrations (with confirmation prompts)
npm run migrate

# Or run directly
ts-node migrations/run_migrations.ts
```

**What happens:**
1. Checks pgvector extension availability
2. Adds `tenant_id` columns (defaults to 'default' for existing data)
3. Converts vectors from BYTEA to pgvector type (in batches)
4. Updates primary keys to include tenant_id
5. Creates HNSW indexes (‚è±Ô∏è **takes 1-2 hours for 10M+ vectors**)

**Monitor progress:**
```sql
-- Check active queries
SELECT pid, query, state, query_start
FROM pg_stat_activity
WHERE datname = 'openmemory';

-- Check index build progress (PostgreSQL 12+)
SELECT
  phase,
  blocks_done,
  blocks_total,
  ROUND(100.0 * blocks_done / NULLIF(blocks_total, 0), 2) AS pct_done
FROM pg_stat_progress_create_index;
```

### Step 4: Update Configuration

Edit your `.env` file:

```bash
# Enable multi-tenancy
OM_MULTI_TENANT=true
OM_DEFAULT_TENANT_ID=default  # Fallback for single-tenant mode

# Enable pgvector
OM_PGVECTOR_ENABLED=true
OM_PGVECTOR_INDEX_TYPE=hnsw   # or ivfflat
OM_PGVECTOR_DISTANCE=cosine   # or l2, ip

# PostgreSQL backend
OM_METADATA_BACKEND=postgres
OM_VECTOR_BACKEND=postgres

# Vector dimensions (match your embedding model)
OM_VEC_DIM=1536  # text-embedding-3-small

# Performance tier
OM_TIER=smart    # or deep for full embeddings
```

### Step 5: Restart and Test

```bash
# Restart OpenMemory server
npm run start

# Test query (via API)
curl -X POST http://localhost:8080/memory/query \
  -H "Content-Type: application/json" \
  -H "x-tenant-id: your-tenant-id" \
  -d '{
    "query": "test search",
    "top_k": 10
  }'
```

**Verify pgvector is working:**
```sql
-- Check that embedding column exists and has data
SELECT COUNT(*)
FROM openmemory_vectors
WHERE embedding IS NOT NULL;

-- Test vector search
SELECT id, embedding <-> '[0.1, 0.2, ...]'::vector AS distance
FROM openmemory_vectors
WHERE tenant_id = 'your-tenant-id' AND sector = 'semantic'
ORDER BY embedding <-> '[0.1, 0.2, ...]'::vector
LIMIT 10;
```

## üèóÔ∏è Multi-Tenant Architecture

### Tenant Isolation

All queries are filtered by `tenant_id`:

```typescript
// Automatic tenant filtering in VectorStore
await vectorStore.searchSimilar(
  sector,
  queryVec,
  topK,
  tenant_id  // ‚Üê Required parameter
);
```

### Tenant ID Extraction

**Option 1: HTTP Header (Default)**
```typescript
// Express middleware
app.use((req, res, next) => {
  req.tenant_id = req.headers['x-tenant-id'] || 'default';
  next();
});
```

**Option 2: JWT Token (Supabase)**
```typescript
import { createClient } from '@supabase/supabase-js';

const supabase = createClient(supabaseUrl, supabaseKey);
const { data: { user } } = await supabase.auth.getUser();
const tenant_id = user.app_metadata.organization_id;
```

**Option 3: API Key Mapping**
```typescript
const API_KEY_TO_TENANT = {
  'sk_live_abc123': 'tenant_company_a',
  'sk_live_xyz789': 'tenant_company_b',
};

const tenant_id = API_KEY_TO_TENANT[apiKey];
```

### Database Schema

After migration, your schema will look like:

```sql
-- Memories table
CREATE TABLE openmemory_memories (
  tenant_id TEXT NOT NULL,
  id UUID NOT NULL,
  user_id TEXT,
  content TEXT,
  primary_sector TEXT,
  -- ... other columns
  PRIMARY KEY (tenant_id, id)
);

-- Vectors table with pgvector
CREATE TABLE openmemory_vectors (
  tenant_id TEXT NOT NULL,
  id UUID NOT NULL,
  sector TEXT NOT NULL,
  embedding vector(1536),  -- ‚Üê pgvector type
  v BYTEA,                 -- ‚Üê Kept for backward compatibility
  dim INTEGER,
  PRIMARY KEY (tenant_id, id, sector)
);

-- HNSW index for fast similarity search
CREATE INDEX idx_vectors_semantic_hnsw
ON openmemory_vectors
USING hnsw (embedding vector_cosine_ops)
WHERE sector = 'semantic';
```

## ‚öôÔ∏è Index Configuration

### HNSW Parameters

Tune these for your use case:

```sql
CREATE INDEX idx ON vectors
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);
```

| Parameter | Default | Description | Impact |
|-----------|---------|-------------|--------|
| `m` | 16 | Connections per layer | Higher = better recall, more memory |
| `ef_construction` | 64 | Build-time candidate list | Higher = better index, slower build |

### Query-time Tuning

```sql
-- Adjust recall vs speed tradeoff
SET hnsw.ef_search = 40;  -- Default: 40, Range: 1-1000
-- Higher = better recall, slower queries
-- Lower = faster queries, lower recall
```

### IVFFlat Alternative

For write-heavy workloads:

```sql
CREATE INDEX idx ON vectors
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 1000);

-- Query-time tuning
SET ivfflat.probes = 10;  -- Default: 1, Range: 1-lists
```

**When to use IVFFlat:**
- High insert rate (> 1000/sec)
- Can tolerate slightly lower recall (90-95% vs 95-99%)
- Limited storage budget

## üìä Performance Optimization

### Connection Pooling

```typescript
const pool = new Pool({
  max: 20,                    // Max connections per instance
  idleTimeoutMillis: 30000,  // Close idle connections after 30s
  connectionTimeoutMillis: 2000,
});
```

**Supabase Pooler:**
- Use connection pooler for serverless: `pooler.supabase.com`
- Transaction mode: Use for read queries
- Session mode: Use for write queries with transactions

### Batch Inserts

```typescript
// Instead of inserting one-by-one:
for (const memory of memories) {
  await addMemory(memory);  // ‚ùå Slow
}

// Use batch inserts:
await Promise.all(
  memories.map(m => addMemory(m))  // ‚úÖ Fast
);
```

### Index Maintenance

```sql
-- Monitor index usage
SELECT
  schemaname,
  tablename,
  indexname,
  idx_scan,
  idx_tup_read,
  idx_tup_fetch
FROM pg_stat_user_indexes
WHERE schemaname = 'public'
ORDER BY idx_scan DESC;

-- Vacuum regularly (auto-vacuum should handle this)
VACUUM ANALYZE openmemory_vectors;

-- Rebuild index if needed
REINDEX INDEX CONCURRENTLY idx_vectors_semantic_hnsw;
```

## üîí Security Best Practices

### Row Level Security (RLS)

For defense-in-depth (optional):

```sql
-- Enable RLS
ALTER TABLE openmemory_memories ENABLE ROW LEVEL SECURITY;
ALTER TABLE openmemory_vectors ENABLE ROW LEVEL SECURITY;

-- Create policy
CREATE POLICY tenant_isolation ON openmemory_memories
  USING (tenant_id = current_setting('app.current_tenant')::text);

CREATE POLICY tenant_isolation ON openmemory_vectors
  USING (tenant_id = current_setting('app.current_tenant')::text);
```

**Set tenant context in application:**
```typescript
await pool.query(
  "SET LOCAL app.current_tenant = $1",
  [tenant_id]
);
```

### API Key Security

```typescript
// Rate limiting per tenant
const limiter = rateLimit({
  keyGenerator: (req) => req.tenant_id,
  max: 100,
  windowMs: 60000,
});

app.use('/memory/*', limiter);
```

## üß™ Testing

### Load Test

```bash
# Install k6
brew install k6  # macOS
# or download from https://k6.io

# Run load test
k6 run load_test.js
```

`load_test.js`:
```javascript
import http from 'k6/http';
import { check } from 'k6';

export const options = {
  vus: 50,  // 50 virtual users
  duration: '5m',
};

export default function () {
  const payload = JSON.stringify({
    query: 'test search query',
    top_k: 10,
  });

  const params = {
    headers: {
      'Content-Type': 'application/json',
      'x-tenant-id': 'tenant_test',
    },
  };

  const res = http.post('http://localhost:8080/memory/query', payload, params);

  check(res, {
    'status is 200': (r) => r.status === 200,
    'response time < 200ms': (r) => r.timings.duration < 200,
  });
}
```

### Query Performance

```sql
-- Analyze query plan
EXPLAIN ANALYZE
SELECT id, embedding <-> '[0.1, 0.2, ...]'::vector AS distance
FROM openmemory_vectors
WHERE tenant_id = 'tenant_123' AND sector = 'semantic'
ORDER BY embedding <-> '[0.1, 0.2, ...]'::vector
LIMIT 10;

-- Should show "Index Scan using idx_vectors_semantic_hnsw"
```

## üêõ Troubleshooting

### Migration Fails

**Error: pgvector extension not found**
```sql
-- Check available extensions
SELECT * FROM pg_available_extensions WHERE name = 'vector';

-- If not found, install pgvector (see Prerequisites)
```

**Error: permission denied**
```sql
-- Grant superuser or rds_superuser role
ALTER USER your_user WITH SUPERUSER;
```

**Error: out of memory during migration**
```sql
-- Reduce batch size in 003_convert_vectors_to_pgvector.sql
batch_size INT := 1000;  -- Default: 10000
```

### Slow Queries

**Check if index is being used:**
```sql
EXPLAIN ANALYZE <your-query>;
-- Look for "Index Scan using hnsw" not "Seq Scan"
```

**Increase ef_search:**
```sql
SET hnsw.ef_search = 100;  -- Default: 40
```

**Rebuild index:**
```sql
REINDEX INDEX CONCURRENTLY idx_vectors_semantic_hnsw;
```

### High Memory Usage

**Reduce index parameters:**
```sql
-- Drop existing index
DROP INDEX idx_vectors_semantic_hnsw;

-- Recreate with lower m
CREATE INDEX idx_vectors_semantic_hnsw
ON openmemory_vectors
USING hnsw (embedding vector_cosine_ops)
WHERE sector = 'semantic'
WITH (m = 8);  -- Default: 16
```

## üìö References

- [pgvector Documentation](https://github.com/pgvector/pgvector)
- [Supabase Vector Guide](https://supabase.com/docs/guides/ai/vector-columns)
- [PostgreSQL Indexing Best Practices](https://www.postgresql.org/docs/current/indexes.html)
- [OpenMemory Architecture](./README.md)

## üÜò Support

If you encounter issues:

1. Check the [Troubleshooting](#-troubleshooting) section
2. Review migration logs for specific errors
3. Open an issue on GitHub with:
   - Error message
   - PostgreSQL version
   - pgvector version
   - Migration step that failed

---

**Next:** [API Usage Guide](./API.md) | [Performance Tuning](./PERFORMANCE.md)
